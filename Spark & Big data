Hadoop processes files in sequencial manner, for random access we can HBASE (NoSQL)
MapReduce vs Spark - MapReduce can only do the batch processing, while spark can do both the batch processing and the real-time processing
Batch processing - storing the data first and then processing 
Real-time processing - processing the data as it comes i.e. immediately (Ex.debit card txn system, it immediately perform all the updates)

=======Why YARN was introduced========
In hadoop 1.0 Job tracker was totally separate machine not namenode
and there is only 1 Job tracker for all the jobs submitted which in turn tracks task trackers of every submitted job (single job can have many task tracker which takes code and runs on worker node)
as no of jobs keep increasing and we have only 1 Job tracker...it becomes performance bottleneck for cluster
We might think that then we can have multiple job trackers, but again we will require something to track all the job trackers which is again bottleneck, but now the bottleneck is in architecture

we can run only mapreduce job, nothing else

If we have 4 slaces of 4GB RAM each, lets say each map job needs 1GB RAM, then if a job comes with 2 map job in that case remaining resource will be wasted i.e 2GB RAM...so basically no resource(RAM, HD, etc) management...Job tracker and task tracker are only related to the job and that too mapreduce job
If a job comes with 10 map, then other 5 map jobs would have to wait in the queue coz single slave has only 4GB RAM

=====How YARN works======
1. When resource mngr receives job, it will contact any NM(node manager) not necessarily the one which contains the data, then that NM(which was contacted by RM) will start an APPLICATION MASTER which will be per job and will monitor everything related to job from start till end.
2.Then APP MASTER will contact it's NM and RM to find the actual data on datanodes and then it will launch containers on those perticular datanodes (container is simply Java or JVM process...inside the container your job will get executed)
3.Now whennever resources are required, AM can called RM for that

In production scenario, it's good to have RM and NameNode on separate machines as they both are masters, but for testing small datasets we can keep them on same machine

RM if not configured has the FIFO scheduler i.e ex if we have 10 nodes in a cluster with 4GB RAM each and assuming 1 map job takes 1GB RAM, if 3-4 ppl come submit 100 map jobs...what will happen, our RAM is 40GB total...req is 100GB, RM will do job scheduling in FIFO manner by default
but sometimes, this FIFO schedulaer may be efficient from priority perspective....so we have FAIR scheduler and CAPACITY scheduler and this 2 are free apache scheduler which will avoid bottleneck of FIFO shceduler...and we can share resources for all the coming jobs no need to waut
Cloudera by default - fair scheduler
Hortonworks by default - capacity scheduler

https://dzone.com/articles/apache-spark-on-yarn-resource-planning.   --->.    executor calculation
https://www.linkedin.com/pulse/how-configure-spark-cluster-yarn-artem-pichugin/

https://stackoverflow.com/questions/32621990/what-are-workers-executors-cores-in-spark-standalone-cluster. -->. spark details

Spark vs MapReduce -- speed comparision --> As spark uses DAG, if it failed at particular step, it can easily go back and start re-processing from there. but, in case of mapReduce it it fails somewhere it has to start from very first step
Other thing is mapReduce does read/write on disk for every map-reduce function and that's why it's slow compare to spark which stores result into memory only as an RDD

PIG & Hive is just another way if you don't know Spark/java/scala/python

We can decide how much % of cluster memory we can give to spark Ex. if cluster has 5 nodes with actual RAM of 4GB, then we have total 20GB and we can allocate for ex. 40% i.e. 8GB to spark cluster.  

Spark tuts --> https://data-flair.training/blogs/what-is-spark/

JVM shouldn't get more than 45GBs of RAM, otherwise garbage collection won't happen properly

no of cores in spark aren't the actual cores of cpu...Ex. if your machine is 8 core...then if you are planning for 6 cores to spark job...then it should 2-3 fold 6...i.e. 12-18 cores (=tasks inside executor)

Settings pref level 
1. code (SparkContext)       most
2. cmd line (spark-submit)    |
3. spark config files         |
4. spark default            least

Spark execution mode ---> very very very very important
fuddi's 6 hr video from 1hr 20min - 2hr 40min

how to choose no of executors for job?
What is shuffle actually? - sending data b/w executors ?...when shuffling happens...join,reduce ops?

Spark job tuning - 
Part 1 - http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/
Part2 - https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/
Important - https://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors
http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/
https://dzone.com/articles/apache-spark-on-yarn-resource-planning
https://dzone.com/articles/alpine-data-how-to-use-the-yarn-api-to-determine-r?fromrel=true
https://dzone.com/articles/understanding-optimized-logical-plan-in-spark?fromrel=true


blog.cloudera.org -- good resource

Spark on YARN - Cloudera blog --> https://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/

Default partitioning?
ES-Spark --> https://www.elastic.co/guide/en/elasticsearch/hadoop/master/arch.html

Serialization
In distributed computing, you generally want to avoid writing data back and forth because it’s expensive. Instead, the common paradigm is to bring your code to your data. This is why many frameworks are based on the JVM, which lets you execute code on the same machine as the data. Serialization is the process of translating this code into an ideally compressed format for efficient transfer over the network. By default, Spark uses the standard Java serializer. However, you can get much faster and more memory-efficient serialization using Kryo serialization. Switching to Kryo can reduce memory pressure on your cluster and improve stability.

What is shuffling and when does it happens...?
https://0x0fff.com/spark-architecture-shuffle/
There are many different tasks that require shuffling of the data across the cluster, for instance table join – to join two tables on the field “id”, you must be sure that all the data for the same values of “id” for both of the tables are stored in the same chunks. Imagine the tables with integer keys ranging from 1 to 1’000’000. By storing the data in same chunks I mean that for instance for both tables values of the key 1-100 are stored in a single partition/chunk, this way instead of going through the whole second table for each partition of the first one, we can join partition with partition directly, because we know that the key values 1-100 are stored only in these two partitions. To achieve this both tables should have the same number of partitions, this way their join would require much less computations. So now you can understand how important shuffling is.

partition's max size?

Spill to memory vs spill to disk (where it spills on disk)

Avoid having exec mem > 64GB (delays in garbage collection)
Running tiny executors (with a single core and just enough memory needed to run a single task, for example) throws away the benefits that come from running multiple tasks in a single JVM

What determines how many tasks can run concurrently on a Spark executor?
Simply,  task per exec = cores per exec
Spark maps the number tasks on a particular Executor to the number of cores allocated to it. By default, Spark assigns one core to a task which is controlled by the spark.task.cpus parameter which defaults to 1

Perfect article about how to have the partitioning in spark application
https://www.dezyre.com/article/how-data-partitioning-in-spark-helps-achieve-more-parallelism/297






